---
title: "StatisticsCoursework"
author: "Alun Meredith"
date: "19 October 2015"
output: pdf_document
---
```{r, dependencies}
library(ggplot2)
library(reshape2)
library(gridExtra)
library(fBasics)
library(RColorBrewer)
library(dplyr)
```

Downloaded fish.txt, data about the catch of a hypothetical fishing fleet from "http://www.edshare.soton.ac.uk/view/courses/COMP6235/2015.html" on `r date()`. 

```{r, read lines}
readLines("fish.txt", 5)
```

From reading the first 5 lines of the file we can see that the data is 2 columns of numerics. Each variable is seperated by a single whitespace, there is no header and no apparent NA characters or comment/escape characters. As such we can use the default values of read.table, including the col.names argument to name the variables.

```{r, import data}
fish <- read.table("fish.txt", col.names = c("times", "size"))
attach(fish)
```

Visualising the data

Produced a scatter graph of the data. 
```{r, step 1}
colours <- brewer.pal(3, "Set1")

clearTheme <- theme(
  panel.background = element_blank(), 
  panel.grid.minor = element_blank(), 
  panel.grid.minor = element_blank())

top <- ggplot() +
  geom_density(aes(times), adjust=0.5) +
  clearTheme +
  geom_vline(xintercept = mean(times), size = 1.2, alpha = 0.5, colour = colours[1], show_guide = T) +
  geom_vline(xintercept = median(times), size = 1.2, alpha = 0.5, colour = colours[2], show_guide = T) + 
  geom_vline(xintercept = exp(mean(log(times))), size = 1.2, alpha = 0.5, colour = colours[3], show_guide = T) + 
  annotate("text", x = median(times)-0.5, y = 0.04, label = "median", colour = colours[2]) +
  annotate("text", x = mean(times)+0.5, y = 0.05, label = "mean", colour = colours[1]) +
  annotate("text", x = exp(mean(log(times)))-0.7, y = 0.03, label = "geometric mean", colour = colours[3])

right <- ggplot() +
  geom_density(aes(size), adjust=0.5) + 
  coord_flip() +
  clearTheme +
  geom_vline(xintercept = mean(size), size = 1.2, alpha = 0.5, colour = colours[1], show_guide = T) +
  geom_vline(xintercept = median(size), size = 1.2, alpha = 0.5, colour = colours[2], show_guide = T) + 
  geom_vline(xintercept = exp(mean(log(size))), size = 1.2, alpha = 0.5, colour = colours[3], show_guide = T)

scatter <- ggplot(data=fish) + 
  geom_point(aes(times,size)) +
  stat_density2d(aes(times,size, fill = ..density..), geom="tile", alpha = 0.4, contour = F) +
  scale_fill_continuous(low = "blue", high = "red")  +
  theme(panel.background = element_blank(),
        legend.position="none")

empty <- ggplot() + 
  geom_point(aes(1,1), colour="white") +
  clearTheme +
  theme(rect=element_blank(), 
        line = element_blank(),
        text = element_blank(),
        title = element_blank())


grid.arrange(top, empty, scatter, right, ncol=2, nrow=2, widths=c(4, 1), heights=c(1, 4))

```

```{r, summary stats}
timesSummary <- round(c(quantile(times),
mean(times),
sqrt(var(times)),
skewness(times),
kurtosis(times),
exp(mean(log(times)))),2)
names(timesSummary) <- c("min", "Q1", "median", "Q3", "max", "mean", "s.dev", "skewness", "kurtosis", "geometric mean")

sizeSummary <- round(c(quantile(size),
mean(size),
sqrt(var(size)),
skewness(size),
kurtosis(size),
exp(mean(log(size)))),2)
names(sizeSummary) <- c("min", "Q1", "median", "Q3", "max", "mean", "s.dev", "skewness", "kurtosis", "geometric mean")

sizeSummary; timesSummary

```

By looking at the above summary statistics and the density distributions in the figure, we can see some interesting features of the distributions. The times fish were caught was very broad. A standard deviation of `r round(sqrt(var(times)),2)`, along side the mean describes a typical value as anywhere in the range: `r round(mean(times) + c(-1,1) * sqrt(var(times)),2)`. The positive skewness which can be seen in the plot is surprising as you would expect the density at 12:01 to be approximately equal to the density at 11:59. This suggests that an effect such as this data being recorded on a Saturday when no fishing is done on a Sunday is occuring, but requires more domain knowledge to analyse fully. 

The distribution of fish sizes shows a distribution which has some signs of being bi-modal. The standard deviation covers `r round(100 * 2*sizeSummary["s.dev"] / (sizeSummary["max"] - sizeSummary["min"]),2)`% of the data range, which is actually greater than the wide flat times data of`r round(100 * 2*timesSummary["s.dev"] / (timesSummary["max"] - timesSummary["min"]),2)`. Both the mean and median sits between the two peaks and there is negligible skewness because the peaks are approximately symmetric and equal in size. 

By assuming an approximation to a normal distribution (t distribution with large degrees of freedom), we can use the t.test function to calcualte confidence intervals. The t.test function makes Bessel's correction automatically.

```{r confidence intervals}
t.test(times)$conf.int
t.test(size)$conf.int
```

We can also map our distributions onto the standard normal curve. The sample distribution mean approximates the mean of the population so we can normalise mean by simply subtracting it. The variance of the sample is given by Bessels correction $$s = \sqrt{\frac{\sum{(x - \bar{x})^2}{n-1}}$$. The variance given by the *var()* function includes Bessel's correction automatically so we just use the *qnorm* function with mean = mean(data) and sd = sqrt(var(data)/n) to compute a confidence interval.

```{r, normal distribution}
qnorm(c(0.95,0.025), mean = mean(times), sd = sqrt(var(times)/length(times)))
```

The central figure above shows a scatter of size of fish and time of day. It is hard to identify any dependencies in the data from this but it looks as though it may be slightly weakly correlated. 
```{r, codependence}
cov(times, fish)
cor(times, fish)
```

You would expect the bigger fish to be easier to catch and therefore some negative correlation to be occuring. The bigger fish will be caught earlier and only the smaller fish left to be caught later. However the correlation is quite small `r round(cor(times,fish)[1,2],2)` and the timescale relatively small we would need mroe domain knowledge to conclude an effect like this. For instance if the fishing is taking place over a larger area (larger population of fish) then the fish caught early in the day will have quite a small effect on the ammount left at the end of the day. 


## Intervals

Splitting the times into 24 intevals corresponding to each hour of the day we can analyse which interval of time has the highest rate of catch (most fish caught that hour) and which has the highest average size of fish. 
```{r, highest intervals}
#which.max(summary(cut(times, 0:25)))
intevals <- lapply(0:23, function(x)
  size[times > x & times <= x+1])
  
names(intevals) <- paste("x",paste(0:23, 1:24, sep = "-"), sep="")

which.max(sapply(intevals, length))
which.max(sapply(intevals, mean))
```

Looking at this data however we can see that the 21:00-22:00 period has the biggest average fish caught is not what was expected considering the negative correlation calcualted. Looking at each inteval however, this is an outlier as the general trend is that the first third of the day averages around 2 and the last half the day is almsot entirely below 2 (with 2 exceptions). 